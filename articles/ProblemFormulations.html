<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="dsdp">
<title>Problem Formulations for `dsdp` • dsdp</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Problem Formulations for `dsdp`">
<meta property="og:description" content="dsdp">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">dsdp</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/ProblemFormulations.html">Problem Formulations for `dsdp`</a>
    <a class="dropdown-item" href="../articles/Tutorial.html">A Tutorial for `dsdp`</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/tsuchiya-lab/dsdp/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Problem Formulations for `dsdp`</h1>
                        <h4 data-toc-skip class="author">Satoshi
Kakihara<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;National Graduate Institute for Policy Studies, &lt;a href="mailto:skakihara@gmail.com" class="email"&gt;skakihara@gmail.com&lt;/a&gt;&lt;/p&gt;'><sup>1</sup></a>
</h4>
                        <h4 data-toc-skip class="author">Takashi
Tsuchiya<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;National Graduate Institute for Policy Studies, &lt;a href="mailto:tsuchiya@grips.ac.jp" class="email"&gt;tsuchiya@grips.ac.jp&lt;/a&gt;&lt;/p&gt;'><sup>2</sup></a>
</h4>
            
            <h4 data-toc-skip class="date">October 26th, 2022</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/tsuchiya-lab/dsdp/blob/HEAD/vignettes/ProblemFormulations.Rmd" class="external-link"><code>vignettes/ProblemFormulations.Rmd</code></a></small>
      <div class="d-none name"><code>ProblemFormulations.Rmd</code></div>
    </div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      <p>This vignette discusses problem formulations used in the
      package <code>dsdp</code>. The main task of <code>dsdp</code> is
      to estimate a density function using a maximum likelihood method
      whose models are a family of exponential distributions with
      polynomial correction terms. In this vignette, we discuss the
      procedure to transform the maximum likelihood problems to a
      variant of semidefinite programming (SDP) problems. Detailed
      discussions of SDP and implementations of solvers will be found in
      another vignette.</p>
    </div>
    
<div class="section level2">
<h2 id="maximum-likelihood-methods">Maximum Likelihood Methods<a class="anchor" aria-label="anchor" href="#maximum-likelihood-methods"></a>
</h2>
<p>Let <span class="math inline">\(g(x)\)</span> be an unknown
univariate density function over the support <span class="math inline">\(S \subset \mathbb{R}\)</span>, where <span class="math inline">\(\mathbb{R}\)</span> denotes a set of real numbers.
For an <span class="math inline">\(n\)</span> data set <span class="math inline">\(\{x_1, \ldots,x_n \}\)</span>, realizations of the
random variable whose density is <span class="math inline">\(g(x)\)</span>, we try to estimate the density
<span class="math inline">\(g\)</span> using the model: <span class="math display">\[\begin{equation}
  f(x; \alpha, \beta) := p(x; \alpha) \cdot K(x; \beta),
  (\#eq:likelihoodfun)
\end{equation}\]</span> where <span class="math inline">\(p(x;
\alpha)\)</span> is a univariate nonnegative polynomial over <span class="math inline">\(S\)</span> with coefficients <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(K(x; \beta)\)</span> is a density function over
the support <span class="math inline">\(S\)</span>. As a base function,
the density <span class="math inline">\(K(x; \beta)\)</span> is an
instance of an exponential family of distributions, namely,</p>
<ol style="list-style-type: decimal">
<li>Gaussian distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[
  N(x;\mu, \sigma^2) := \frac{1}{\sigma \sqrt{2\pi}}
  \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right), \quad x \in S =
(-\infty, \infty).
  \]</span>
</li>
<li>An exponential distribution with rate parameter <span class="math inline">\(\lambda &gt; 0\)</span> <span class="math display">\[
  \mathrm{Exp}(x;\lambda) := \lambda e^{-\lambda x}, \quad x \in S = [0,
\infty).
  \]</span>
</li>
</ol>
<p>In the sequel, we call Gaussian distribution with a polynomial
<strong>Gaussian-based model</strong> and an exponential distribution
with a polynomial <strong>Exponential-based model</strong>,
respectively.</p>
<p>One of the approaches for this problem is a maximum likelihood method
(MLE). In MLEs, to estimate the model with a data set <span class="math inline">\(\{x_1, \ldots,x_n \}\)</span>, it maximizes the
product of the probabilities @ref(eq:likelihoodfun) as follows: <span class="math display">\[\begin{equation}
\max_{\alpha, \beta} \prod_{i=1}^{n} f(x_i; \alpha, \beta),
(\#eq:mleprod)
\end{equation}\]</span> or, in the form of a log likelihood: <span class="math display">\[\begin{equation}
\max_{\alpha, \beta} \sum_{i=1}^{n} \log f(x_i; \alpha, \beta).
(\#eq:mlelog)
\end{equation}\]</span> The principle of MLE is that good parameters, in
this case, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, maximize the probability of
realization of observed samples <span class="math inline">\(\{x_1,
\ldots,x_n \}\)</span>. The log likelihood @ref(eq:mlelog) is also
concerned with a maximum entropy principle, and they are discussed in,
for example, <span class="citation">(<a href="#ref-akaike1998information" role="doc-biblioref">Akaike 1973</a>;
<a href="#ref-konishi2008information" role="doc-biblioref">Konishi and
Kitagawa 2008</a>)</span>.</p>
<p>As for a constraint condition, we need to guarantee that <span class="math inline">\(f\)</span> is a probability density: <span class="math display">\[\begin{equation}
  \int_{S} p(x; \alpha) \cdot K(x; \beta) \mathrm{d}x = 1.
(\#eq:constraint0)
\end{equation}\]</span></p>
<p>In computation, it is customary to take a minus of a log likelihood,
which flips the sign of the objective and change a maximization problem
to a minimization problem. In addition, we introduce a regularization
term <span class="math inline">\(\mathrm{rAIC}(\alpha, \beta)\)</span>
to avoid overfitting. We will discuss AIC later in detail.</p>
<p>The skeleton of the estimation problem becomes: <span class="math display">\[\begin{equation}
\begin{aligned}
\min_{\alpha, \beta}&amp; -\sum_{i=1}^{n} \left[\log p(x_i; \alpha) +
\log K(x_i; \beta)\right]
  + \mathrm{rAIC}(\alpha, \beta)\\
\text{s.t.  }&amp;
\begin{array}{l}
p(x;\alpha) \ge 0, \quad x \in S,\\
\int_{S} p(x; \alpha) \cdot K(x; \beta) \mathrm{d}x = 1.
\end{array}
\end{aligned}
(\#eq:optskelton)
\end{equation}\]</span></p>
<p>To compare different models, we need to consider some regularization
regarding the complexities of models; otherwise complex models get
better in specialization than simpler ones, but complex models fail in
generalization. In our case, we adopt Akaike Information Criterion (AIC)
<span class="citation">(<a href="#ref-akaike1974new" role="doc-biblioref">Akaike 1974</a>)</span>. With AIC, the
regularization term <span class="math inline">\(\mathrm{rAIC}(\alpha,
\beta)\)</span> are put on the objective of @ref(eq:optskelton), the
adjusted which penalizes the number of free parameters, i.e., the number
of parameters minus the number of constraints.</p>
<p>In the following cases, considering the fact that the number of
parameters of coefficient is <span class="math inline">\(k+1\)</span>
for polynomials of degree <span class="math inline">\(k\)</span>, the
number of parameters of Gaussian distribution is 2, and that of an
exponential distribution is 1, and the number of the constraints in
@ref(eq:optskelton), with the presence of @ref(eq:constraint0), is 1, we
have</p>
<ol style="list-style-type: decimal">
<li>Gaussian-based model <span class="math display">\[
\mathrm{rAIC}(\alpha, \mu, \sigma^2) =  k + 2.
\]</span>
</li>
<li>Exponential-based model <span class="math display">\[
\mathrm{rAIC}(\alpha, \lambda) =  k + 1.
\]</span>
</li>
</ol>
<!-- Since it is difficult to find best $\alpha$ and $\beta$ simultaneously,
we solve this problem in two-stage processes; we first solve the problem 
in regard to $\alpha$ for fixed $\beta$, and then choose good $\beta$
among the candidates. --><p>In practice, we restrict the set of degrees <span class="math inline">\(\Theta_{\mathrm{deg}}\)</span> and the set of
parameters of base functions <span class="math inline">\(\Theta_\beta\)</span> to be finite cardinality,
and compute the coefficients <span class="math inline">\(\alpha\)</span>
from these values using SDP.
<!-- and AIC for each degree of polynomials and
$\beta\in\Theta_{\beta}$, to find the best model. --></p>
<p>Our estimation @ref(eq:optskelton) is now more concisely presented as
follows:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\min_{\begin{subarray}{l}
      k \in\Theta_{\mathrm{deg}}\\
      \beta\in\Theta_{\beta}
      \end{subarray}}
  \min_{\alpha\in\mathbb{R}^{k+1}}&amp; -\sum_{i=1}^{n}
    \left[\log p(x_i; \alpha) + \log K(x_i; \beta)\right] +
\mathrm{rAIC}(\alpha, \beta)\\
\text{s.t.  }&amp;
\begin{array}{l}
p(x;\alpha) \ge 0, \quad x \in S,\\
\int_{S} p(x; \alpha) \cdot K(x; \beta) \mathrm{d}x = 1,
\end{array}
\end{aligned}
(\#eq:optskelton2)
\end{equation}\]</span> where <span class="math inline">\(\Theta_{\mathrm{deg}}\)</span> and <span class="math inline">\(\Theta_{\beta}\)</span> are finite sets.</p>
<!-- In practice,  we first provide the reasonable set of $\Theta_d$ and $\Theta_\beta$, and then compute
the coefficient $\alpha$,  -->
</div>
<div class="section level2">
<h2 id="semidefinite-programming-problems">Semidefinite Programming Problems<a class="anchor" aria-label="anchor" href="#semidefinite-programming-problems"></a>
</h2>
<p>This section is the main topic of this vignette where we discuss the
estimation of coefficients of polynomials with fixed degrees of
polynomials and parameters of base functions.</p>
<p>Before that, we introduce several notations. For a positive integer
<span class="math inline">\(d\)</span>, we define a (d+1)-dimensional
column vector <span class="math inline">\(\textbf{x}_{d} := (1,
x,\ldots, x^d)^T\)</span> for <span class="math inline">\(x \in
\mathbb{R}\)</span>, where the superscript <span class="math inline">\(\square^T\)</span> denotes the transpose of a
vector or a matrix, and also define a (d+1) by (d+1) matrix <span class="math inline">\(X_d := \textbf{x}_{d}
\textbf{x}_{d}^T\)</span>.</p>
<p><span class="math inline">\(Q, Q_1, Q_2\)</span> and <span class="math inline">\(Q_3\)</span> denote symmetric matrices with
appropriate sizes, and <span class="math inline">\(Q \succeq 0\)</span>
denotes the semidefiniteness of the symmetric matrix <span class="math inline">\(Q\)</span>, i.e., all eigenvalues of <span class="math inline">\(Q\)</span> are nonnegative. <span class="math inline">\(\mathrm{trace}(Q)\)</span> denotes the trace of
the matrix <span class="math inline">\(Q\)</span>, i.e., the sum of the
diagonal elements of <span class="math inline">\(Q\)</span>.</p>
<p>Using these notations, nonnegativity of k-th order polynomials is
expressed in quadratic forms as follows.</p>
<div id="unnamed-chunk-1" class="proposition">
<p>(e.g. <span class="citation">(<a href="#ref-nesterov2000squared" role="doc-biblioref">Nesterov 2000</a>; <a href="#ref-fushiki2006maximum" role="doc-biblioref">Fushiki, Horiuchi,
and Tsuchiya 2006</a>)</span>) Let <span class="math inline">\(p(x;
\alpha)\)</span> be a univariate k-th degree polynomial with coefficient
<span class="math inline">\(\alpha\)</span>. Then, there exists a unique
quadratic form corresponding to <span class="math inline">\(\alpha\)</span> in each of the following
cases.</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(p(x; \alpha) \ge 0\)</span> over <span class="math inline">\(S = (-\infty, \infty)\)</span>
</li>
</ol>
<ul>
<li>
<span class="math inline">\(k\)</span> is even and <span class="math inline">\(d = \frac{k}{2}\)</span> <span class="math display">\[
p(x; \alpha) = \textbf{x}_{d}^T Q \textbf{x}_{d} = \mathrm{trace}(X_d
Q),
\]</span> for some unique symmetric matrix <span class="math inline">\(Q
\succeq 0\)</span>.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>
<span class="math inline">\(p(x; \alpha) \ge 0\)</span> over <span class="math inline">\(S = [0, \infty)\)</span>
</li>
</ol>
<ul>
<li>
<span class="math inline">\(k\)</span> is odd and <span class="math inline">\(d = \frac{k-1}{2}\)</span> <span class="math display">\[
p(x;\alpha) = \textbf{x}_d^T Q_1 \textbf{x}_d + x \cdot \textbf{x}_d^T
Q_2
\textbf{x}_d = \mathrm{trace}(X_d Q_1) + \mathrm{trace}(x X_d Q_2)
\]</span> for some unique symmetric matrices <span class="math inline">\(Q_1, Q_2 \succeq 0\)</span>.</li>
<li>
<span class="math inline">\(k\)</span> is even and <span class="math inline">\(d = \frac{k}{2}\)</span> <span class="math display">\[
p(x;\alpha) = \textbf{x}_d^T Q_1 \textbf{x}_d + x \cdot
\textbf{x}_{d-1}^T Q_3
\textbf{x}_{d-1} = \mathrm{trace}(X_d Q_1) + \mathrm{trace}(x X_{d-1}
Q_3),
\]</span> for some unique symmetric matrices <span class="math inline">\(Q_1, Q_3 \succeq 0\)</span>.</li>
</ul>
</div>
<p>Using this proposition, the positivity constraint
@ref(eq:constraint0) with k-th order polynomials (<span class="math inline">\(k \ge 1\)</span>) is now written down to as
follows.</p>
<ol style="list-style-type: decimal">
<li>Gaussian-based model</li>
</ol>
<ul>
<li>
<span class="math inline">\(k\)</span> is even and <span class="math inline">\(d = \frac{k}{2}\)</span> <span class="math display">\[
  \begin{aligned}
  1&amp;=\int_{-\infty}^{\infty}p(x;\alpha)N(x;\mu, \sigma^2)\mathrm{d}x
=
   \int_{-\infty}^{\infty} \textbf{x}_d^T Q \textbf{x}_d \cdot
   N(x;\mu, \sigma^2) \mathrm{d}x \\
  &amp;= \mathrm{trace}\left(\int_{-\infty}^{\infty}Q  \textbf{x}_d
\textbf{x}_d^T
  N(x;\mu,\sigma^2)\mathrm{d}x \right)
  = \mathrm{trace}\left(Q \int_{-\infty}^{\infty} X_d N(x;\mu, \sigma^2)
  \mathrm{d}x\right) \\
   &amp;= \mathrm{trace}(Q M),
  \end{aligned}
  \]</span> where <span class="math inline">\(M :=
\int_{-\infty}^{\infty} X_{d} N(x; \mu, \sigma^2)
\mathrm{d}x\)</span>.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Exponential-based model</li>
</ol>
<ul>
<li>
<span class="math inline">\(k\)</span> is odd and <span class="math inline">\(d = \frac{k-1}{2}\)</span> <span class="math display">\[
\begin{aligned}
1&amp;=\int_{0}^{\infty} p(x;\alpha)\mathrm{Exp}(x;\lambda)\mathrm{d}x =
\int_{0}^{\infty}
(\textbf{x}_d^T Q_1 \textbf{x}_d +
    x \cdot \textbf{x}_{d}^T Q_2 \textbf{x}_{d}) \cdot
\mathrm{Exp}(x;\lambda) \mathrm{d}x \\
&amp;= \mathrm{trace}\left(\int_{0}^{\infty}Q_1  \textbf{x}_d
\textbf{x}_d^T
\mathrm{Exp}(x;\lambda)\mathrm{d}x \right) +
\mathrm{trace}\left(\int_{0}^{\infty}Q_2  x \textbf{x}_d \textbf{x}_d^T
\mathrm{Exp}(x;\lambda)\mathrm{d}x \right)\\
&amp;= \mathrm{trace}\left(Q_1 \int_{0}^{\infty} X_d
\mathrm{Exp}(x;\lambda)\mathrm{d}x \right) +
\mathrm{trace}\left(Q_2 \int_{0}^{\infty} x X_d
\mathrm{Exp}(x;\lambda)\mathrm{d}x \right) \\
&amp;= \mathrm{trace}(Q_1 M_1) + \mathrm{trace}(Q_2 M_2).
\end{aligned}
\]</span> where <span class="math inline">\(M_1 := \int_{0}^{\infty}
X_{d} \mathrm{Exp}(x; \lambda) \mathrm{d}x\)</span>, <span class="math inline">\(M_2 := \int_{0}^{\infty} x X_{d} \mathrm{Exp}(x;
\lambda) \mathrm{d}x\)</span>,</li>
<li>
<span class="math inline">\(k\)</span> is even and <span class="math inline">\(d = \frac{k}{2}\)</span> <span class="math display">\[
\begin{aligned}
1&amp;=\int_{0}^{\infty}p(x;\alpha)\mathrm{Exp}(x;\lambda)\mathrm{d}x \\
&amp;=\mathrm{trace}\left(Q_1 \int_{0}^{\infty} X_d
\mathrm{Exp}(x;\lambda)\mathrm{d}x \right) +
\mathrm{trace}\left(Q_2 \int_{0}^{\infty} x X_{d-1}
\mathrm{Exp}(x;\lambda)\mathrm{d}x \right) \\
&amp;= \mathrm{trace}(Q_1 M_1) + \mathrm{trace}(Q_2 M_3),
\end{aligned}
\]</span> where <span class="math inline">\(M_1 := \int_{0}^{\infty}
X_{d} \mathrm{Exp}(x; \lambda) \mathrm{d}x\)</span>, <span class="math inline">\(M_3 := \int_{0}^{\infty} x X_{d-1} \mathrm{Exp}(x;
\lambda) \mathrm{d}x\)</span>.</li>
</ul>
<p>Note that <span class="math inline">\(M\)</span> is a moment matrix
whose (i, j)-th element is an (i+j-2)-th moment of Gaussian
distribution, and similarly, an (i, j)-th element of <span class="math inline">\(M_1\)</span> is an (i+j-2)-th moment of an
exponential distribution, and an (i, j)-th element of <span class="math inline">\(M_2\)</span> or <span class="math inline">\(M_3\)</span> is an (i+j-1)-th moment of the same
distribution.</p>
<p>We briefly summarize the computation of the moments. Using the moment
generating function <span class="math inline">\(M_{X}(t) =
E[e^{tX}]\)</span> of the random variable <span class="math inline">\(X\)</span>, where <span class="math inline">\(E\)</span> is an expectation operator, k-th degree
of the moment <span class="math inline">\(m_k\)</span> is obtained by
differentiating <span class="math inline">\(M_{X}(t)\)</span> with
respect to <span class="math inline">\(t\)</span> k times at <span class="math inline">\(t=0\)</span>, i.e., <span class="math display">\[
m_k := \left. \frac{d^k M_{X}(t)}{dt^k}\right|_{t=0}.
\]</span> Thus, for each of the cases, its k-th moment is obtained as
follows.</p>
<ol style="list-style-type: decimal">
<li>Gaussian distribution <span class="math inline">\(N(x; \mu,
\sigma^2)\)</span><br>
The moment generating function is <span class="math inline">\(M(t) =
e^{\mu t + \frac{1}{2}\sigma^2 t^2}\)</span>, and its k-th moment is
<span class="math display">\[\begin{equation}
  m_k = \left. \frac{d^k e^{\mu t + \frac{1}{2}\sigma^2
t^2}}{dt^k}\right|_{t=0}.
  (\#eq:momentgauss)
  \end{equation}\]</span> For example, we show k-th moments for <span class="math inline">\(k=0,1,2,3,4,5\)</span> in the table below</li>
</ol>
<table class="table">
<thead><tr class="header">
<th>k</th>
<th align="right">k-th moment(<span class="math inline">\(m_k\)</span>)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td>1</td>
<td align="right"><span class="math inline">\(\mu\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td align="right"><span class="math inline">\(\mu^2+\sigma^2\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td align="right"><span class="math inline">\(\mu^3+3\mu\sigma^2\)</span></td>
</tr>
<tr class="odd">
<td>4</td>
<td align="right"><span class="math inline">\(\mu^4+6\mu^2\sigma^2+3\sigma^4\)</span></td>
</tr>
<tr class="even">
<td>5</td>
<td align="right"><span class="math inline">\(\mu^5+10\mu^3\sigma^2+15\mu\sigma^4\)</span></td>
</tr>
</tbody>
</table>
<ol start="2" style="list-style-type: decimal">
<li>An exponential distribution <span class="math inline">\(\mathrm{Exp}(x;\lambda) = \lambda \exp(-\lambda
x)\)</span><br>
The moment generating function is <span class="math inline">\(M(t) =
\frac{\lambda}{\lambda - t}, \text{ for } t &lt; \lambda\)</span>, and
its k-th moment is <span class="math display">\[\begin{equation}
m_k = \frac{k!}{\lambda^k}.
  (\#eq:momentexp)
  \end{equation}\]</span>
</li>
</ol>
<p>Note that since the moment becomes easily too large as <span class="math inline">\(k\)</span> becomes larger, a moment matrix tends
to be highly ill conditioned for large degrees of polynomials, which
makes the computation of coefficients difficult.</p>
<p>Before we show SDP formulations of density estimation, we introduce
auxiliary variables <span class="math inline">\(y_i = p(x_i;\alpha),
i=1,\ldots,n\)</span> and data matrices <span class="math inline">\(X^{(1)}, \ldots, X^{(n)}\)</span> generated by the
elements of the data set <span class="math inline">\(x_1, \ldots,
x_n\)</span> as described in the beginning of the section.</p>
<p>Finally MLE @ref(eq:optskelton2) with a data set <span class="math inline">\(x_1, \ldots, x_n\)</span> becomes as follows.</p>
<ol style="list-style-type: decimal">
<li>Gaussian-based Model<br>
Let <span class="math inline">\(\Theta_{\mathrm{deg}}\)</span> be a
finite set of positive even integer, <span class="math inline">\(\Theta_{\mu}\)</span> be a finite set of real
numbers, and <span class="math inline">\(\Theta_{\sigma}\)</span> be a
finite set of positive real numbers. <span class="math display">\[
\min_{
   \begin{subarray}{l}
   k \in\Theta_{\mathrm{deg}}\\
   \mu\in\Theta_{\mu}\\
   \sigma\in\Theta_{\sigma}
   \end{subarray}}
\mathrm{SDP}_{\mathrm{Gauss}}(k, \mu, \sigma;x_1,\ldots,x_n)
\]</span> with <span class="math inline">\(\mathrm{SDP}_{\mathrm{Gauss}}(k, \mu,
\sigma;x_1,\ldots,x_n)\)</span>: <span class="math display">\[\begin{equation}
  \begin{aligned}
  \min_{
  \begin{subarray}{c}
  y_1,\ldots,y_n\\
  Q
  \end{subarray}}
&amp;-\sum_{i=1}^{n} \left[ \log y_i + \log N(x_i; \mu, \sigma^2)\right]
+ (k+2)\\
  \text{s.t. }&amp;
  \begin{array}{l}
  y_i =\mathrm{trace}(X_{d}^{(i)} Q),\\
  y_i \ge 0, \quad i=1,\ldots,n,\\
  \mathrm{trace}(M Q) = 1,\\
  Q \succeq 0,
  \end{array}
  \end{aligned}
  (\#eq:gausssdp)
  \end{equation}\]</span> where <span class="math inline">\(d =
\frac{k}{2}\)</span>, <span class="math inline">\(Q \in
\mathbb{R}^{(d+1)\times(d+1)}\)</span> is a symmetric matrix, and <span class="math inline">\(M\)</span> is the moment matrix whose (i,j)
element is <span class="math inline">\(m_{i+j-2}\)</span> in
@ref(eq:momentgauss).</li>
<li>Exponential-based model<br>
Let <span class="math inline">\(\Theta_{deg}\)</span> be a finite set of
positive integers, and <span class="math inline">\(\Theta_{\lambda}\)</span> be a finite set of
positive real numbers. <span class="math display">\[
\min_{
   \begin{subarray}{l}
   k \in\Theta_{\mathrm{deg}}\\
   \lambda\in\Theta_{\lambda}
   \end{subarray}}
\mathrm{SDP}_{\mathrm{Exp}}(k, \lambda;x_1,\ldots,x_n)
\]</span> with <span class="math inline">\(\mathrm{SDP}_{\mathrm{Exp}}(k,
\lambda;x_1,\ldots,x_n)\)</span>:</li>
</ol>
<ul>
<li>
<span class="math inline">\(k\)</span> is odd <span class="math display">\[\begin{equation}
\begin{aligned}
\min_{
\begin{subarray}{c}
y_1,\ldots,y_n\\
Q_1, Q_2
\end{subarray}}
&amp;-\sum_{i=1}^{n} \left[ \log y_i +\log \mathrm{Exp}(x_i;
\lambda)\right] + (k+1)\\
\text{s.t. } &amp;
\begin{array}{l}
y_i = \mathrm{trace}(X_{d}^{(i)} Q_1) + \mathrm{trace}(x_i X_{d}^{(i)}
Q_2),\\
y_i \ge 0, \quad i=1,\ldots,n,\\
\mathrm{trace}(M_1 Q_1) + \mathrm{trace}(M_2 Q_2) = 1,\\
Q_1 \succeq 0,\\
Q_2 \succeq 0,
\end{array}
\end{aligned}
(\#eq:expsdp1)
  \end{equation}\]</span> where <span class="math inline">\(d =
\frac{k-1}{2}\)</span>, <span class="math inline">\(Q_1, Q_2 \in
\mathbb{R}^{(d+1)\times(d+1)}\)</span> are symmetric matrices, and <span class="math inline">\(M_1, M_2 \in
\mathbb{R}^{(d+1)\times(d+1)}\)</span> are the moment matrices whose
(i,j) element are <span class="math inline">\(m_{i+j-2}\)</span> and
<span class="math inline">\(m_{i+j-1}\)</span> in @ref(eq:momentexp),
respectively.</li>
<li>
<span class="math inline">\(k\)</span> is even <span class="math display">\[\begin{equation}
\begin{aligned}
\min_{
\begin{subarray}{c}
y_1,\ldots,y_n\\
Q_1,Q_3
\end{subarray}}
&amp;-\sum_{i=1}^{n} \left[ \log y_i +\log \mathrm{Exp}(x_i;
\lambda)\right] + (k+1)\\
\text{s.t. } &amp;
\begin{array}{l}
y_i = \mathrm{trace}(X_{d}^{(i)} Q_1) + \mathrm{trace}(x_i X_{d-1}^{(i)}
Q_3),\\
y_i \ge 0, \quad i=1,\ldots,n\\
\mathrm{trace}(M_1 Q_1) + \mathrm{trace}(M_3 Q_3) = 1,\\
Q_1 \succeq 0,\\
Q_3 \succeq 0,
\end{array}
\end{aligned}
(\#eq:expsdp0)
\end{equation}\]</span> where <span class="math inline">\(d =
\frac{k}{2}\)</span>, <span class="math inline">\(Q_1 \in
\mathbb{R}^{(d+1)\times(d+1)}\)</span> and <span class="math inline">\(Q_3 \in \mathbb{R}^{d\times d}\)</span> are
symmetric matrices, and <span class="math inline">\(M_1 \in
\mathbb{R}^{(d+1)\times(d+1)}\)</span> and <span class="math inline">\(M_3 \in \mathbb{R}^{d\times d}\)</span> are the
moment matrices whose (i,j) element are <span class="math inline">\(m_{i+j-2}\)</span> and <span class="math inline">\(m_{i+j-1}\)</span> in @ref(eq:momentexp),
respectively.</li>
</ul>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-akaike1998information" class="csl-entry">
Akaike, Hirotugu. 1973. <span>“Information Theory and an Extension of
the Maximum Likelihood Principle.”</span> In <em>Second International
Symposium in Information Theory</em>, edited by B. N. Petrov and F.
Caski, 267–81. Budapest: Akademiai Kiado.
</div>
<div id="ref-akaike1974new" class="csl-entry">
———. 1974. <span>“A New Look at the Statistical Model
Identification.”</span> <em>IEEE Transactions on Automatic Control</em>
19 (6): 716–23.
</div>
<div id="ref-fushiki2006maximum" class="csl-entry">
Fushiki, Tadayoshi, Shingo Horiuchi, and Takashi Tsuchiya. 2006.
<span>“A Maximum Likelihood Approach to Density Estimation with
Semidefinite Programming.”</span> <em>Neural Computation</em> 18 (11):
2777–2812.
</div>
<div id="ref-konishi2008information" class="csl-entry">
Konishi, Sadanori, and Genshiro Kitagawa. 2008. <span>“Information
Criteria and Statistical Modeling.”</span>
</div>
<div id="ref-nesterov2000squared" class="csl-entry">
Nesterov, Yurii. 2000. <span>“Squared Functional Systems and
Optimization Problems.”</span> In <em>High Performance
Optimization</em>, 405–40. Springer.
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Satoshi Kakihara, Takashi Tsuchiya.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
